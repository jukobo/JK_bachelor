{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AE_functions import *\n",
    "from make_dataset import *\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try, simple NN\n",
    "\n",
    "Training on one image with Adam optimizer (lr = 1e-5) and AdamW optimizer (lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=12288, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=12288, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 0.24112845957279205\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 0.0009576617740094662\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 7.66075900173746e-05\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 7.13144027031376e-06\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 9.908711717798724e-07\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 1.9889847635568003e-07\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 0.0009235304896719754\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 4.11888504459057e-06\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 2.1446123810164863e-07\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 2.3866252263360366e-07\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 2.4431566103544355e-08\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 1.0987742937729195e-09\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 6.036898203554131e-10\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 1.9874452084067684e-10\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 7.713752553373254e-11\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 5.837216821014124e-10\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 0.0010506970575079322\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 1.8189495676779188e-05\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 6.646533279308642e-07\n",
      "\n",
      "EVALUATION!\n",
      "Validation loss: 1.606266550879809e-07\n"
     ]
    }
   ],
   "source": [
    "#Define paramters\n",
    "parameters_dict = {\n",
    "    'epochs': 500,\n",
    "    'learning_rate': 1e-3, #NOTE - change here\n",
    "    'batch_size': 1, \n",
    "    'weight_decay': 5e-4 \n",
    "}\n",
    "\n",
    "## Unpack parameters\n",
    "num_epochs = parameters_dict['epochs']\n",
    "lr = parameters_dict['learning_rate']\n",
    "batch_size = parameters_dict['batch_size']\n",
    "wd = parameters_dict['weight_decay']\n",
    "\n",
    "\n",
    "## Loading data\n",
    "\n",
    "img_dir_training = \"C:/Users/julie/Bachelor_data/crops_training_prep/img\"\n",
    "heatmap_dir_training = \"C:/Users/julie/Bachelor_data/crops_training_prep/heatmaps\"\n",
    "msk_dir_training = \"C:/Users/julie/Bachelor_data/crops_training_prep/msk\"\n",
    "\n",
    "\n",
    "VerSe_train = LoadData(img_dir=img_dir_training, msk_dir = msk_dir_training, distfield_dir=heatmap_dir_training)\n",
    "train_loader = DataLoader(VerSe_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    # 39 elements (images) in train_loader\n",
    "    # Each element is a tuple of 3 elements: (img, heatmap, msk)\n",
    "    # img: torch.Size([2, 128, 128, 96])\n",
    "\n",
    "input_train, y, z = train_loader.dataset[10]\n",
    "# plt.imshow(input_train[0][64, :, :], cmap='gray')\n",
    "# plt.title('Original')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "## Define model\n",
    "# For simple AE\n",
    "model = AE([128*96, 512, 256, 128])\n",
    "# model = AE([128*96, 512, 256, 128, 64])\n",
    "# model = AE([128*96, 2*512, 2*256, 2*128, 2*64])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "\n",
    "o_loss = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "recon_img = []\n",
    "recon_img.append(input_train[0][64,:,:].numpy())\n",
    "recon_img_name = []\n",
    "recon_img_name.append('Original')\n",
    "\n",
    "## Train model\n",
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    step = -1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        overall_loss = 0\n",
    "\n",
    "        x = input_train[0][64,:,:].unsqueeze(dim=0)\n",
    "        x = x.view(1, -1)\n",
    "        x = x.to(device)\n",
    "\n",
    "        x_reconstructed = model(x)\n",
    "\n",
    "        #-- Loss function\n",
    "        loss = loss_function(x_reconstructed, x)\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss. backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update step\n",
    "        step+=1\n",
    "\n",
    "        # Do evaluation every 50 epoch\n",
    "        if step%25 == 0:\n",
    "            print()\n",
    "            print(\"EVALUATION!\")\n",
    "            model.eval() #Set to evaluation\n",
    "\n",
    "            #Training evaluation\n",
    "            val_loss_eval = []\n",
    "            with torch.no_grad():\n",
    "                inputs = input_train[0][64,:,:].unsqueeze(dim=0)\n",
    "                inputs = inputs.view(1, -1)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                inputs_reconstructed = model(inputs)\n",
    "                \n",
    "                #-- Loss function\n",
    "                v_loss = loss_function(inputs_reconstructed, inputs)\n",
    "\n",
    "                #-- Save image\n",
    "                if step%50 == 0:\n",
    "                    recon_img.append(inputs_reconstructed.detach().cpu().numpy().reshape(128, 96))\n",
    "                    recon_img_name.append('Reconstructed image, '+str(step)+' epochs')\n",
    "\n",
    "\n",
    "                # Save loss\n",
    "                val_loss_eval.append(v_loss.item())\n",
    "            avg_loss_val = np.mean(val_loss_eval)\n",
    "            print(\"Validation loss: \"+str(avg_loss_val))\n",
    "            val_loss.append(avg_loss_val)\n",
    "\n",
    "\n",
    "        o_loss.append(overall_loss)  \n",
    "    \n",
    "\n",
    "train(model, optimizer, num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged towards 0.00011297758966003738\n"
     ]
    }
   ],
   "source": [
    "## Plotting losses from training\n",
    "\n",
    "print(f\"Converged towards {np.mean(o_loss[350:])}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(f'Model Loss, batch_size=1, lr={lr}, wd=0.0005')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Avg. loss')\n",
    "ax.set_xticks(np.arange(0, len(o_loss), step= 25))\n",
    "\n",
    "ax.plot(list(range(1, len(o_loss)+1, 1)), o_loss, label='Training loss', color='b')  # Update the plot with the current loss\n",
    "ax.plot(list(range(25, len(o_loss)+1, 25)), val_loss, label='Validation loss', color='r')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting reconstructed images\n",
    "\n",
    "fig, ax  = plt.subplots(2, 3, figsize=(12, 15))\n",
    "ii = [0, 1, 2, 3, 6, 10]\n",
    "\n",
    "for i in range(1,6):\n",
    "    ax[i//3, i%3].imshow(recon_img[ii[i]], cmap='gray')\n",
    "    ax[i//3, i%3].set_title(recon_img_name[ii[i]])\n",
    "\n",
    "\n",
    "ax[0,0].imshow(input_train[0][64, :, :], cmap='gray')\n",
    "ax[0,0].set_title('Original Image')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.290683808250151e-09\n"
     ]
    }
   ],
   "source": [
    "## Make reconstruction\n",
    "\n",
    "model.eval()\n",
    "input_train, y, z = train_loader.dataset[10]\n",
    "org_img = input_train[0][64,:,:].unsqueeze(dim=0)\n",
    "x = org_img.view(1, -1)\n",
    "\n",
    "x = x.to(device)\n",
    "x_reconstructed = model(x) \n",
    "print(f'loss={loss_function(x_reconstructed, x)}')\n",
    "\n",
    "\n",
    "x_reconstructed = x_reconstructed.detach().cpu().numpy().reshape(128, 96)\n",
    "\n",
    "\n",
    "## Plotting the difference from outlier input to reconstructions\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(org_img.squeeze(), cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "ax[1].imshow(x_reconstructed, cmap='gray')\n",
    "ax[1].set_title('Reconstructed Image')\n",
    "\n",
    "ax[2].imshow(org_img.squeeze() - x_reconstructed, cmap='bwr')\n",
    "ax[2].set_title('Difference')\n",
    "fig.colorbar(ax[2].imshow(org_img.squeeze() - x_reconstructed, cmap='bwr'))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
