{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader \n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from AE_functions import *\n",
    "from eval_functions import *\n",
    "from make_dataset import * \n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define which training model, that should be evaluated\n",
    "\n",
    "train = '53625'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/53625_training/reconstruction0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m      8\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_training/reconstruction\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mno[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Normalize the image, so it fits the float32 format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/53625_training/reconstruction0.npy'"
     ]
    }
   ],
   "source": [
    "## Plotting the reconstructions\n",
    "\n",
    "no = [0, 0,50, 100, 250, 975] # Change which image, which have to be shown\n",
    "\n",
    "fig, ax  = plt.subplots(2, 3, figsize=(12, 15))\n",
    "\n",
    "for i in range(1,6):\n",
    "    file = f'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/{train}_training/reconstruction{no[i]}.npy'\n",
    "    x = np.load(file)\n",
    "\n",
    "    x.astype(np.float64)\n",
    "\n",
    "    # Normalize the image, so it fits the float32 format\n",
    "    x = x[0,:,:]\n",
    "    img_float32 = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    ax[i//3, i%3].imshow(img_float32, cmap='gray')\n",
    "    ax[i//3, i%3].set_title(f'Reconstructed Image, {no[i]} epochs')\n",
    "\n",
    "\n",
    "file = f'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/{train}_training/original.npy'\n",
    "x = np.load(file)\n",
    "x.astype(np.float64)\n",
    "x = x[0,:,:]\n",
    "img_float32 = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "ax[0,0].imshow(img_float32, cmap='gray')\n",
    "ax[0,0].set_title('Original Image')\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.savefig(f'C:/Users/julie/OneDrive/Skrivebord/merged_{train}.png')  # Save the plot as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged towards 0.00016877190224366152\n"
     ]
    }
   ],
   "source": [
    "## Plotting losses from training\n",
    "\n",
    "file_loss = f'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/A_Trainings/{train}_training/o_loss.npy'\n",
    "file_val_loss = f'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/A_Trainings/{train}_training/Val_loss.npy'\n",
    "\n",
    "o_loss = np.load(file_loss)\n",
    "val_loss = np.load(file_val_loss)\n",
    "print(f\"Converged towards {np.mean(o_loss[250:])}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(f'Model Loss, batch_size=780, lr=0.001, wd=0.0005')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Avg. loss')\n",
    "# ax.set_xticks(np.arange(0, len(o_loss), step= 100))\n",
    "ax.set_xticks(np.arange(0, len(o_loss), step= 200))\n",
    "\n",
    "ax.plot(list(range(1, len(o_loss)+1, 1)), o_loss, label='Training loss', color='b')  # Update the plot with the current loss\n",
    "# ax.plot(list(range(25, len(o_loss)+1, 25)), val_loss, label='Validation loss', color='r')\n",
    "# ax.plot(list(range(3, len(o_loss)+1, 4)), val_loss[:-62], label='Validation loss', color='r')\n",
    "ax.plot(list(range(20, len(o_loss)+1, 20)), val_loss, label='Validation loss', color='r')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig(f'C:/Users/julie/OneDrive/Skrivebord/model_loss_{train}.png')  # Save the plot as a PNG file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LoadData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m heatmap_dir_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/julie/Bachelor_data/crops_validation_prep/heatmaps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m msk_dir_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/julie/Bachelor_data/crops_validation_prep/msk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m VerSe_test \u001b[38;5;241m=\u001b[39m \u001b[43mLoadData\u001b[49m(img_dir\u001b[38;5;241m=\u001b[39mimg_dir_test, msk_dir \u001b[38;5;241m=\u001b[39m msk_dir_test, distfield_dir\u001b[38;5;241m=\u001b[39mheatmap_dir_test)\n\u001b[0;32m      9\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(VerSe_test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m healthy, outlier \u001b[38;5;241m=\u001b[39m healthy_outlier(test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LoadData' is not defined"
     ]
    }
   ],
   "source": [
    "## Loading test image + outlier image\n",
    "\n",
    "# Loading data (testset, 41 elements)\n",
    "img_dir_test = \"C:/Users/julie/Bachelor_data/crops_validation_prep/img\"\n",
    "heatmap_dir_test = \"C:/Users/julie/Bachelor_data/crops_validation_prep/heatmaps\"\n",
    "msk_dir_test = \"C:/Users/julie/Bachelor_data/crops_validation_prep/msk\"\n",
    "\n",
    "VerSe_test = LoadData(img_dir=img_dir_test, msk_dir = msk_dir_test, distfield_dir=heatmap_dir_test)\n",
    "test_loader = DataLoader(VerSe_test, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "healthy, outlier = healthy_outlier(test_loader)\n",
    "print(len(healthy), len(outlier))\n",
    "# Create datasets\n",
    "dataset_healthy = generate_dataset(healthy, 120)\n",
    "dataset_outlier = generate_dataset_outlier(outlier, 120, radius=30, mode=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset_healthy[0][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the model with new data (healthy and outliers)\n",
    "\n",
    "# Loading the model\n",
    "model_path = f'C:/Users/julie/OneDrive/Skrivebord/Bachelor/JK_bachelor/A_Trainings/{train}_training/model_conv_1999.pth'\n",
    "\n",
    "model = load_model(model_path, [1, 8, 16, 32, 64])\n",
    "\n",
    "\n",
    "# Testing the model with MSE\n",
    "error_healthy = []\n",
    "error_outlier = []\n",
    "\n",
    "for i in range(len(dataset_healthy)):\n",
    "    # test_healthy, _, _  = test_loader.dataset[i]\n",
    "    test_healthy = dataset_healthy[i]\n",
    "    loss = evaluate_model(model, test_healthy[0].unsqueeze(dim=0))\n",
    "    error_healthy.append(loss)\n",
    "\n",
    "    # test_outlier, _, _ = test_loader_o.dataset[i]\n",
    "    test_outlier = dataset_outlier[i]\n",
    "    loss = evaluate_model(model, test_outlier[0].unsqueeze(dim=0))\n",
    "    error_outlier.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make histogram\n",
    "\n",
    "\n",
    "hist_values1, hist_values2, bins1, bins2 = plot_histograms(error_healthy, error_outlier, 10)\n",
    "\n",
    "mean_healthy = np.mean(error_healthy)\n",
    "std_healthy = np.std(error_healthy)\n",
    "\n",
    "mean_outlier = np.mean(error_outlier)\n",
    "std_outlier = np.std(error_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean healthy: 0.0004540509785632215, std healthy: 0.00047075337899920113\n",
      "Mean outlier: 0.0007210962678072975, std outlier: 0.0007537231888396763\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean healthy: {mean_healthy}, std healthy: {std_healthy}\")\n",
    "print(f\"Mean outlier: {mean_outlier}, std outlier: {std_outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the difference from outlier input to reconstructions\n",
    "org_img = dataset_outlier[0].squeeze()\n",
    "reconstructed_img = model(dataset_outlier[0].unsqueeze(dim=0)).squeeze().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(dataset_outlier[0].squeeze(), cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "ax[1].imshow(reconstructed_img, cmap='gray')\n",
    "ax[1].set_title('Reconstructed Image')\n",
    "\n",
    "ax[2].imshow(org_img - reconstructed_img, cmap='bwr')\n",
    "ax[2].set_title('Difference')\n",
    "fig.colorbar(ax[2].imshow(org_img - reconstructed_img, cmap='bwr'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the model with new data (healthy and outliers)\n",
    "\n",
    "# Testing the model\n",
    "ssim_healthy = []\n",
    "ssim_outlier = []\n",
    "\n",
    "for i in range(len(dataset_healthy)):\n",
    "    # test_healthy, _, _  = test_loader.dataset[i]\n",
    "    test_healthy = dataset_healthy[i]\n",
    "    loss = evaluate_model_SSIM(model, test_healthy[0].unsqueeze(dim=0))\n",
    "    ssim_healthy.append(loss)\n",
    "\n",
    "    # test_outlier, _, _ = test_loader_o.dataset[i]\n",
    "    test_outlier = dataset_outlier[i]\n",
    "    loss = evaluate_model_SSIM(model, test_outlier[0].unsqueeze(dim=0))\n",
    "    ssim_outlier.append(loss)\n",
    "\n",
    "hist_values1, hist_values2, bins1, bins2 = plot_histograms_SSIM(ssim_healthy, ssim_outlier, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean structural similarity index over the image is: 0.9831384337692096\n"
     ]
    }
   ],
   "source": [
    "## Plotting the difference from outlier input to reconstructions\n",
    "\n",
    "org_img = dataset_outlier[0].squeeze()\n",
    "org_img_np = org_img.cpu().detach().numpy()\n",
    "\n",
    "reconstructed_img = model(dataset_outlier[0].unsqueeze(dim=0)).squeeze().detach().numpy()\n",
    "reconstructed_img_np = reconstructed_img.astype(np.float64)\n",
    "\n",
    "\n",
    "# Compute SSIM\n",
    "mssim, ssim_map  = ssim(org_img_np, reconstructed_img_np, full=True) # full=True returns the full structural similarity image\n",
    "print('The mean structural similarity index over the image is:',mssim)\n",
    "#plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(org_img_np, cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "ax[1].imshow(reconstructed_img_np, cmap='gray')\n",
    "ax[1].set_title('Reconstructed Image')\n",
    "\n",
    "ax[2].imshow(ssim_map, cmap='bwr')\n",
    "ax[2].set_title('SSIM Map')\n",
    "fig.colorbar(ax[2].imshow(ssim_map, cmap='bwr'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect latent representations for healthy and outlier data\n",
    "latent_representations_healthy = collect_latent_representations(model, dataset_healthy)\n",
    "latent_representations_outlier = collect_latent_representations(model, dataset_outlier)\n",
    "# print(latent_representations_healthy[0].shape)\n",
    "\n",
    "\n",
    "# Flatten the collected latent representations\n",
    "latent_representations_healthy_flat = np.concatenate([latent.flatten().reshape(1, -1) for latent in latent_representations_healthy], axis=0)\n",
    "latent_representations_outlier_flat = np.concatenate([latent.flatten().reshape(1, -1) for latent in latent_representations_outlier], axis=0)\n",
    "\n",
    "merged = np.concatenate((latent_representations_healthy_flat, latent_representations_outlier_flat), axis=0)\n",
    "target = np.zeros((merged.shape[0], 1))\n",
    "target[120:] = 1\n",
    "\n",
    "target2 = []\n",
    "for i in range(merged.shape[0]):\n",
    "    if i < 120:\n",
    "        target2.append('Healthy')\n",
    "    else:\n",
    "        target2.append('Outlier')\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(target2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18953142 0.25426736 0.30560464 0.34805405 0.38748682 0.41621858\n",
      " 0.44307014 0.46698776 0.4866832  0.5055224  0.52170014 0.535875\n",
      " 0.5495374  0.5624044  0.57505035 0.5863494  0.5974953  0.6077024\n",
      " 0.6172996  0.6266624  0.6355506  0.64380044 0.65193427 0.6597651\n",
      " 0.6671287  0.6740743  0.6807611  0.6873211  0.69382393 0.69991297\n",
      " 0.70593065 0.7115652  0.71702725 0.7223506  0.7275716  0.73254293\n",
      " 0.7373642  0.74208176 0.7466772  0.75112754 0.75552243 0.7597821\n",
      " 0.76395124 0.76795506 0.7717683  0.7755602  0.7792517  0.7828778\n",
      " 0.7864152  0.78993595 0.7934135  0.79677117 0.8000928  0.80330485\n",
      " 0.8064563  0.809516   0.8124417  0.8153263  0.818142   0.8209096\n",
      " 0.8236459  0.8263357  0.8289593  0.83157396 0.8341256  0.8366277\n",
      " 0.8391174  0.84150594 0.84385043 0.84614354 0.8483829  0.85059834\n",
      " 0.8527861  0.85490525 0.8569895  0.85906714 0.86109704 0.8630902\n",
      " 0.8650403  0.86695516 0.86885655 0.8707203  0.87257105 0.8743971\n",
      " 0.87618697 0.87794137 0.87966895 0.8813872  0.88307035 0.88474375\n",
      " 0.8864075  0.88802534 0.8896211  0.8911893  0.8927486  0.8942944\n",
      " 0.8958191  0.8972998  0.8987679  0.9002124  0.9016506  0.9030771\n",
      " 0.9044737  0.90584517 0.90719527 0.9085349  0.90985376 0.9111506\n",
      " 0.9124393  0.9137091  0.9149665  0.91620845 0.9174349  0.91864765\n",
      " 0.919846   0.9210283  0.9221896  0.9233419  0.92447686 0.9256072\n",
      " 0.9267203  0.92782974 0.9289136  0.9299813  0.93104184 0.93208617\n",
      " 0.93312573 0.93415755 0.9351714  0.9361804  0.9371756  0.9381568\n",
      " 0.9391374  0.9400987  0.94105846 0.9420098  0.94294864 0.9438812\n",
      " 0.94480664 0.94571245 0.94661266 0.9475047  0.9483796  0.9492494\n",
      " 0.9501153  0.95096576 0.95180804 0.95264554 0.9534679  0.9542872\n",
      " 0.9551013  0.95590794 0.9567063  0.9575015  0.9582852  0.9590569\n",
      " 0.9598201  0.9605732  0.96131575 0.96205217 0.96278167 0.96350646\n",
      " 0.9642186  0.96492875 0.96562797 0.96632147 0.96700877 0.96768934\n",
      " 0.9683613  0.96903026 0.9696902  0.9703414  0.9709794  0.9716115\n",
      " 0.9722371  0.9728586  0.97347564 0.9740832  0.9746833  0.97527707\n",
      " 0.97586906 0.9764528  0.9770262  0.9775928  0.9781571  0.97871464\n",
      " 0.9792689  0.97981584 0.98035836 0.9808986  0.98143226 0.9819607\n",
      " 0.9824826  0.9829964  0.98350525 0.9840073  0.98450047 0.98499197\n",
      " 0.9854658  0.98593444 0.9863999  0.98686284 0.98731434 0.9877566\n",
      " 0.98819435 0.98862773 0.98905617 0.98948276 0.989899   0.9903118\n",
      " 0.99071676 0.9911171  0.9915155  0.9919079  0.99229753 0.99268264\n",
      " 0.9930629  0.99343216 0.99379814 0.9941624  0.9945246  0.99487716\n",
      " 0.9952289  0.9955704  0.9959076  0.99623996 0.9965666  0.9968879\n",
      " 0.99720114 0.9975123  0.99781835 0.99811435 0.99840534 0.99869126\n",
      " 0.99897164 0.99924356 0.9995034  0.9997581  0.99999976 0.99999976]\n"
     ]
    }
   ],
   "source": [
    "## Explained Variance\n",
    "\n",
    "pca = PCA().fit(merged)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.title('PCA explained variance')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2) \n",
    "# pca.fit(merged)\n",
    "pca.fit(latent_representations_healthy_flat)\n",
    "\n",
    "# Transform\n",
    "latent_pca = pca.transform(merged)\n",
    "\n",
    "\n",
    "## Plotting the PCA visualization\n",
    "color_mapping = {'Healthy': 'blue', 'Outlier': 'red'}\n",
    "colors = [color_mapping[label] for label in target2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(latent_pca[:, 0], latent_pca[:, 1], c=colors, alpha=0.5) #, cmap='viridis')\n",
    "\n",
    "plt.title('PCA Visualization of Latent Space Representations')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Healthy', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Outlier', markersize=10, markerfacecolor='red')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(merged)\n",
    "# pca.fit(latent_representations_healthy_flat)\n",
    "\n",
    "#Transform\n",
    "latent_pca = pca.transform(merged)\n",
    "\n",
    "## Plotting the PCA visualization\n",
    "color_mapping = {'Healthy': 'blue', 'Outlier': 'red'}\n",
    "colors = [color_mapping[label] for label in target2]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(latent_pca[:, 0], latent_pca[:, 1], latent_pca[:, 2], c=colors, alpha=0.5)\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Healthy', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Outlier', markersize=10, markerfacecolor='red')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "ax.set_title('PCA Visualization of Latent Space Representations (3D)')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.legend(handles=legend_elements)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=40, init='pca', learning_rate=100, n_iter=10000)\n",
    "\n",
    "# Transform\n",
    "latent_tsne = tsne.fit_transform(merged)\n",
    "\n",
    "## Plotting the t-SNE visualization\n",
    "color_mapping = {'Healthy': 'blue', 'Outlier': 'red'}\n",
    "colors = [color_mapping[label] for label in target2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=colors, alpha=0.5) #, cmap='viridis')\n",
    "\n",
    "plt.title('TSNE Visualization of Latent Space Representations')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Healthy', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Outlier', markersize=10, markerfacecolor='red')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tsne with 3 components\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=30, learning_rate=100) \n",
    "\n",
    "# Transform\n",
    "latent_tsne = tsne.fit_transform(merged)\n",
    "\n",
    "## Plotting the t-SNE visualization\n",
    "color_mapping = {'Healthy': 'blue', 'Outlier': 'red'}\n",
    "colors = [color_mapping[label] for label in target2]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(latent_tsne[:, 0], latent_tsne[:, 1], latent_tsne[:, 2], c=colors, alpha=0.5)\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Healthy', markersize=10, markerfacecolor='blue'),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Outlier', markersize=10, markerfacecolor='red')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "ax.set_title('TSNE Visualization of Latent Space Representations (3D)')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.set_zlabel('t-SNE 3')\n",
    "ax.legend(handles=legend_elements)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
